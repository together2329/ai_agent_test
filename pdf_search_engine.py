"""
PDF ë¬¸ì¥ ê²€ìƒ‰ ì—”ì§„ - í•µì‹¬ ê¸°ëŠ¥
"""

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import PyPDF2
import pdfplumber
import re
import nltk
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
import logging
from dataclasses import dataclass
import json
from datetime import datetime

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    page_number: int
    similarity_score: float
    start_char: int
    end_char: int
    context_before: str = ""
    context_after: str = ""

class PDFSearchEngine:
    """PDF ë¬¸ì¥ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self, 
                 model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',
                 chunk_size: int = 500,
                 chunk_overlap: int = 50):
        
        self.model_name = model_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        
        # ë¬¸ì„œ ì €ì¥
        self.document_chunks = []
        self.chunk_embeddings = None
        self.pdf_metadata = {}
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.embedding_dim})")
        
    def load_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """PDF íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬"""
        pdf_path = Path(pdf_path)
        
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
            
        print(f"ğŸ“„ PDF ë¡œë”© ì¤‘: {pdf_path.name}")
        
        # PDF ë©”íƒ€ë°ì´í„°
        self.pdf_metadata = {
            "filename": pdf_path.name,
            "path": str(pdf_path),
            "loaded_at": datetime.now().isoformat()
        }
        
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pages_text = self._extract_text_from_pdf(pdf_path)
        
        # ì²­í‚¹
        self.document_chunks = self._create_chunks(pages_text)
        
        # ì„ë² ë”© ìƒì„±
        self._generate_embeddings()
        
        stats = {
            "total_pages": len(pages_text),
            "total_chunks": len(self.document_chunks),
            "avg_chunk_length": np.mean([len(chunk['content']) for chunk in self.document_chunks]),
            "filename": pdf_path.name
        }
        
        print(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: {stats['total_pages']}í˜ì´ì§€, {stats['total_chunks']}ê°œ ì²­í¬")
        
        return stats
        
    def _extract_text_from_pdf(self, pdf_path: Path) -> List[Dict[str, Any]]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        pages_text = []
        
        try:
            # pdfplumber ì‚¬ìš© (ë” ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    text = page.extract_text()
                    if text:
                        pages_text.append({
                            "page_number": page_num,
                            "content": text,
                            "char_count": len(text)
                        })
                        
        except Exception as e:
            # PyPDF2ë¡œ fallback
            logger.warning(f"pdfplumber ì‹¤íŒ¨, PyPDF2ë¡œ ì¬ì‹œë„: {e}")
            
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    text = page.extract_text()
                    if text:
                        pages_text.append({
                            "page_number": page_num,
                            "content": text,
                            "char_count": len(text)
                        })
                        
        return pages_text
        
    def _create_chunks(self, pages_text: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        
        for page_data in pages_text:
            page_num = page_data["page_number"]
            text = page_data["content"]
            
            # ë¬¸ì¥ ë¶„í• 
            sentences = nltk.sent_tokenize(text)
            
            # ì²­í¬ ìƒì„±
            current_chunk = ""
            current_sentences = []
            
            for sentence in sentences:
                # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€ ì‹œ í¬ê¸° í™•ì¸
                potential_chunk = current_chunk + " " + sentence if current_chunk else sentence
                
                if len(potential_chunk.split()) <= self.chunk_size:
                    current_chunk = potential_chunk
                    current_sentences.append(sentence)
                else:
                    # í˜„ì¬ ì²­í¬ ì €ì¥
                    if current_chunk:
                        chunks.append({
                            "content": current_chunk.strip(),
                            "page_number": page_num,
                            "sentences": current_sentences.copy(),
                            "word_count": len(current_chunk.split()),
                            "char_start": text.find(current_sentences[0]) if current_sentences else 0,
                            "char_end": text.find(current_sentences[-1]) + len(current_sentences[-1]) if current_sentences else 0
                        })
                    
                    # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© ê³ ë ¤)
                    if self.chunk_overlap > 0 and current_sentences:
                        overlap_sentences = current_sentences[-self.chunk_overlap:]
                        current_chunk = " ".join(overlap_sentences) + " " + sentence
                        current_sentences = overlap_sentences + [sentence]
                    else:
                        current_chunk = sentence
                        current_sentences = [sentence]
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
            if current_chunk:
                chunks.append({
                    "content": current_chunk.strip(),
                    "page_number": page_num,
                    "sentences": current_sentences,
                    "word_count": len(current_chunk.split()),
                    "char_start": text.find(current_sentences[0]) if current_sentences else 0,
                    "char_end": text.find(current_sentences[-1]) + len(current_sentences[-1]) if current_sentences else 0
                })
                
        return chunks
        
    def _generate_embeddings(self):
        """ì²­í¬ë“¤ì˜ ì„ë² ë”© ìƒì„±"""
        if not self.document_chunks:
            return
            
        print(f"ğŸ”„ {len(self.document_chunks)}ê°œ ì²­í¬ì˜ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        texts = [chunk["content"] for chunk in self.document_chunks]
        self.chunk_embeddings = self.model.encode(texts, 
                                                show_progress_bar=True,
                                                convert_to_numpy=True)
        
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {self.chunk_embeddings.shape}")
        
    def search(self, 
               query: str, 
               top_k: int = 5,
               min_similarity: float = 0.1) -> List[SearchResult]:
        """ë¬¸ì¥ ê²€ìƒ‰ ìˆ˜í–‰"""
        
        if not self.document_chunks or self.chunk_embeddings is None:
            raise ValueError("PDFë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")
            
        print(f"ğŸ” ê²€ìƒ‰ ì¤‘: '{query}'")
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.model.encode(query, convert_to_numpy=True)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = self._compute_similarities(query_embedding, self.chunk_embeddings)
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ì„ íƒ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            similarity = similarities[idx]
            
            if similarity < min_similarity:
                continue
                
            chunk = self.document_chunks[idx]
            
            # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            context_before, context_after = self._get_context(idx)
            
            result = SearchResult(
                content=chunk["content"],
                page_number=chunk["page_number"],
                similarity_score=float(similarity),
                start_char=chunk.get("char_start", 0),
                end_char=chunk.get("char_end", 0),
                context_before=context_before,
                context_after=context_after
            )
            
            results.append(result)
            
        print(f"âœ… {len(results)}ê°œ ê²°ê³¼ ì°¾ìŒ")
        return results
        
    def _compute_similarities(self, query_embedding: np.ndarray, 
                            doc_embeddings: np.ndarray) -> np.ndarray:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ì •ê·œí™”
        query_norm = query_embedding / np.linalg.norm(query_embedding)
        doc_norms = doc_embeddings / np.linalg.norm(doc_embeddings, axis=1, keepdims=True)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarities = np.dot(doc_norms, query_norm)
        return similarities
        
    def _get_context(self, chunk_idx: int, context_size: int = 1) -> Tuple[str, str]:
        """ì£¼ë³€ ì²­í¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ"""
        context_before = ""
        context_after = ""
        
        # ì´ì „ ì²­í¬
        if chunk_idx > 0:
            for i in range(max(0, chunk_idx - context_size), chunk_idx):
                if self.document_chunks[i]["page_number"] == self.document_chunks[chunk_idx]["page_number"]:
                    context_before += self.document_chunks[i]["content"] + " "
                    
        # ë‹¤ìŒ ì²­í¬  
        if chunk_idx < len(self.document_chunks) - 1:
            for i in range(chunk_idx + 1, min(len(self.document_chunks), chunk_idx + context_size + 1)):
                if self.document_chunks[i]["page_number"] == self.document_chunks[chunk_idx]["page_number"]:
                    context_after += self.document_chunks[i]["content"] + " "
                    
        return context_before.strip(), context_after.strip()
        
    def get_statistics(self) -> Dict[str, Any]:
        """ë¬¸ì„œ í†µê³„ ì •ë³´"""
        if not self.document_chunks:
            return {}
            
        stats = {
            "filename": self.pdf_metadata.get("filename", "Unknown"),
            "total_chunks": len(self.document_chunks),
            "total_pages": max(chunk["page_number"] for chunk in self.document_chunks),
            "avg_chunk_length": np.mean([chunk["word_count"] for chunk in self.document_chunks]),
            "total_words": sum(chunk["word_count"] for chunk in self.document_chunks),
            "pages_distribution": {}
        }
        
        # í˜ì´ì§€ë³„ ì²­í¬ ë¶„í¬
        for chunk in self.document_chunks:
            page = chunk["page_number"]
            if page not in stats["pages_distribution"]:
                stats["pages_distribution"][page] = 0
            stats["pages_distribution"][page] += 1
            
        return stats
        
    def save_search_results(self, query: str, results: List[SearchResult], 
                          output_path: str = "results/search_results.json"):
        """ê²€ìƒ‰ ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ê²°ê³¼ ì§ë ¬í™”
        serialized_results = []
        for result in results:
            serialized_results.append({
                "content": result.content,
                "page_number": result.page_number,
                "similarity_score": result.similarity_score,
                "start_char": result.start_char,
                "end_char": result.end_char,
                "context_before": result.context_before,
                "context_after": result.context_after
            })
            
        # ê²€ìƒ‰ ê¸°ë¡
        search_record = {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "pdf_metadata": self.pdf_metadata,
            "results_count": len(results),
            "results": serialized_results
        }
        
        # íŒŒì¼ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(search_record, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥: {output_path}") 